{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd16 ChaosAI: \u5947\u5999\u306a\u82f1\u8a9e\u6587\u3092\u751f\u6210\u3059\u308b\u5c0f\u578bAI\u30e2\u30c7\u30eb\n", "\n", "\u3053\u306eColab\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001TinyGPT-2\uff08\u307e\u305f\u306fGPT2-small\uff09\u3092\u4f7f\u7528\u3057\u3066\u3001\n", "\u5947\u5999\u3067\u610f\u5473\u4e0d\u660e\u306a\u82f1\u8a9e\u306e\u6587\u3092\u751f\u6210\u3059\u308bAI\u3092LoRA\u3067\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059\u3002"]}, {"cell_type": "code", "metadata": {}, "source": ["# \u2705 \u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n", "!pip install transformers datasets peft accelerate bitsandbytes"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# \u2705 \u30e2\u30c7\u30eb\u3068Tokenizer\u306e\u8aad\u307f\u8fbc\u307f\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "\n", "model_name = 'gpt2'\n", "tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# \u2705 \u30c7\u30fc\u30bf\u306e\u6e96\u5099\uff08\u4f8b\u3068\u3057\u3066\u5947\u5999\u306a\u82f1\u8a9e\u6587\u3092\u76f4\u63a5\u66f8\u304f\uff09\n", "train_data = [\n", "    {'text': \"The broccoli rebelled against the moon's gravity.\"},\n", "    {'text': \"I drank the alphabet and coughed out philosophy.\"},\n", "    {'text': \"Yesterday, a spoon declared itself president of clouds.\"},\n", "    {'text': \"The sidewalk laughed when I stepped on an idea.\"},\n", "    {'text': \"Time forgot to tick, so I slept through reality.\"},\n", "]\n", "\n", "from datasets import Dataset\n", "dataset = Dataset.from_list(train_data)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# \u2705 LoRA\u306e\u6e96\u5099\u3068\u5b66\u7fd2\u8a2d\u5b9a\n", "from peft import get_peft_model, LoraConfig, TaskType\n", "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n", "\n", "# LoRA\u8a2d\u5b9a\n", "lora_config = LoraConfig(\n", "    task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1, bias=\"none\"\n", ")\n", "\n", "model = get_peft_model(model, lora_config)\n", "\n", "# Tokenize\n", "def tokenize(example):\n", "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)\n", "\n", "tokenized_dataset = dataset.map(tokenize)\n", "\n", "# \u5b66\u7fd2\u5f15\u6570\n", "training_args = TrainingArguments(\n", "    output_dir=\"output\",\n", "    per_device_train_batch_size=2,\n", "    num_train_epochs=3,\n", "    logging_steps=1,\n", "    save_steps=10,\n", "    save_total_limit=1,\n", "    fp16=True,\n", ")\n", "\n", "# \u30c8\u30ec\u30fc\u30ca\u30fc\n", "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_dataset,\n", "    tokenizer=tokenizer,\n", "    data_collator=data_collator,\n", ")\n", "\n", "trainer.train()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# \u2705 \u30c6\u30b9\u30c8\u751f\u6210\n", "input_text = \"The chair decided\"\n", "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n", "outputs = model.generate(**inputs, max_new_tokens=50)\n", "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 2}